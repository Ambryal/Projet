Nom du Fichier :

Das_Martins

Titre :

A Survey on Automatic Text Summarization

Auteurs :

Dipanjan Das Andre F.T. MartinsLanguage Technologies Institute Carnegie Mellon University fdipanjan, 21, 2007

Courriels :

afmg@cs.cmu.eduNovember

Abstract :

The increasing availability of online information has necessitated intensive research in the area of automatic text summarization within the Natural Lan- guage Processing (NLP) community. Over the past half a century, the prob- lem has been addressed from many dierent perspectives, in varying domains and using various paradigms. This survey intends to investigate some of the most relevant approaches both in the areas of single-document and multiple- document summarization, giving special emphasis to empirical methods and extractive techniques. Some promising approaches that concentrate on specic details of the summarization problem are also discussed. Special attention is devoted to automatic evaluation of summarization systems, as future research on summarization is strongly dependent on progress in this area.1	 IntroductionThe subeld of summarization has been investigated by the NLP community for nearly the last half century. Radev et al. (2002) dene a summary as \a text that is produced from one or more texts, that conveys important information in the original text(s), and that is no longer than half of the original text(s) and usually signicantly less than that". This simple denition captures three important aspectsthat characterize research on automatic summarization:• Summaries may be produced from a single document or multiple documents,• Summaries should preserve important information,• Summaries should be short.Even if we agree unanimously on these points, it seems from the literature that any attempt to provide a more elaborate denition for the task would result in disagreement within the community. In fact, many approaches dier on the manner of their problem formulations. We start by introducing some common terms in thesummarization dialect: extraction is the procedure of identifying important sectionsof the text and producing them verbatim; abstraction aims to produce important material in a new way; fusion combines extracted parts coherently; and compression aims to throw out unimportant sections of the text (Radev et al., 2002).Earliest instances of research on summarizing scientic documents proposed       paradigms for extracting salient sentences from text using features like word and phrase frequency (Luhn, 1958), position in the text (Baxendale, 1958) and key phrases (Edmundson, 1969). Various work published since then has concentrated on         other domains, mostly on newswire data. Many approaches addressed the problemby building systems depending of the type of the required summary. While extractive summarization is mainly concerned with what the summary content should be, usu- ally relying solely on extraction of sentences, abstractive summarization puts strong emphasis on the form, aiming to produce a grammatical summary, which usually requires advanced language generation techniques. In a paradigm more tuned to information retrieval (IR), one can also consider topic-driven summarization, that assumes that the summary content depends on the preference of the user and canbe assessed via a query, making the nal summary focused on a particular topic.A crucial issue that will certainly drive future research on summarization is evaluation. During the last fteen years, many system evaluation competitions like TREC,See http://trec.nist.gov/. DUCSee http://duc.nist.gov/. and MUCSee http://www.itl.nist.gov/iad/894.02/related projects/muc/proceedings/.muc 7 toc.html have created sets of training material and have estab- lished baselines for performance levels. However, a universal strategy to evaluate summarization systems is still absent.In this survey, we primarily aim to investigate how empirical methods have been used to build summarization systems. The rest of the paper is organized as fol- lows: Section 2 describes single-document summarization, focusing on extractive techniques. Section 3 progresses to discuss the area of multi-document summariza- tion, where a few abstractive approaches that pioneered the eld are also considered. Section 4 briey discusses some unconventional approaches that we believe can be useful in the future of summarization research. Section 5 elaborates a few eval- uation techniques and describes some of the standards for evaluating summaries automatically. Finally, Section 6 concludes the survey.2	 Single-Document SummarizationUsually, the ow of information in a given document is not uniform, which meansthat some parts are more important than others. The major challenge in summa- rization lies in distinguishing the more informative parts of a document from theless ones. Though there have been instances of research describing the automatic creation of abstracts, most work presented in the literature relies on verbatim ex- traction of sentences to address the problem of single-document summarization. Inthis section, we describe some eminent extractive techniques. First, we look at early work from the 1950s and 60s that kicked o research on summarization. Second, we concentrate on approaches involving machine learning techniques published in the 1990s to today. Finally, we briey describe some techniques that use a more complex natural language analysis to tackle the problem.2.1 Early WorkMost early work on single-document summarization focused on technical documents. Perhaps the most cited paper on summarization is that of (Luhn, 1958), that de- scribes research done at IBM in the 1950s. In his work, Luhn proposed that the frequency of a particular word in an article provides an useful measure of its sig- nicance. There are several key ideas put forward in this paper that have assumed importance in later work on summarization. As a rst step, words were stemmed to their root forms, and stop words were deleted. Luhn then compiled a list of content words sorted by decreasing frequency, the index providing a signicance measure of the word. On a sentence level, a signicance factor was derived that reects the number of occurrences of signicant words within a sentence, and the linear distance between them due to the intervention of non-signicant words. All sentences are ranked in order of their signicance factor, and the top ranking sentences are nally selected to form the auto-abstract.Related work (Baxendale, 1958), also done at IBM and published in the same journal, provides early insight on a particular feature helpful in nding salient parts of documents: the sentence position. Towards this goal, the author examined 200 paragraphs to nd that in 85% of the paragraphs the topic sentence came as the rst one and in 7% of the time it was the last sentence. Thus, a naive but fairly accurateway to select a topic sentence would be to choose one of these two. This positional feature has since been used in many complex machine learning based systems.Edmundson (1969) describes a system that produces document extracts. His primary contribution was the development of a typical structure for an extractive summarization experiment. At rst, the author developed a protocol for creating manual extracts, that was applied in a set of 400 technical documents. The two features of word frequency and positional importance were incorporated from the previous two works. Two other features were used: the presence of cue words (presence of words like signicant, or hardly ), and the skeleton of the document (whether the sentence is a title or heading). Weights were attached to each of these features manually to score each sentence. During evaluation, it was found that about44% of the auto-extracts matched the manual extracts.2.2 Machine Learning MethodsIn the 1990s, with the advent of machine learning techniques in NLP, a series of semi- nal publications appeared that employed statistical techniques to produce document extracts. While initially most systems assumed feature independence and relied on naive-Bayes methods, others have focused on the choice of appropriate features and on learning algorithms that make no independence assumptions. Other signicant approaches involved hidden Markov models and log-linear models to improve ex- tractive summarization. A very recent paper, in contrast, used neural networks and third party features (like common words in search engine queries) to improve purely extractive single document summarization. We next describe all these approaches in more detail.2.2.1 Naive-Bayes MethodsKupiec et al. (1995) describe a method derived from Edmundson (1969) that is ableto learn from data. The classication function categorizes each sentence as worthyof extraction or not, using a naive-Bayes classier. Let s be a particular sentence,S the set of sentences that make up the summary, and F1;:::;Fk the features. Assuming independence of the features:Qki=1 P (Fi j s 2 S ) P (s 2 S )P (s 2 S j F1;F2;::Fk) = Qki=1 P (Fi) (1)The features were compliant to (Edmundson, 1969), but additionally included the sentence length and the presence of uppercase words. Each sentence was given a score according to (1), and only the n top sentences were extracted. To evaluate the system, a corpus of technical documents with manual abstracts was used in the following way: for each sentence in the manual abstract, the authors manually analyzed its match with the actual document sentences and created a mapping (e.g. exact match with a sentence, matching a join of two sentences, not matchable, etc.). The auto-extracts were then evaluated against this mapping. Feature analysis revealed that a system using only the position and the cue features, along with the sentence length sentence feature, performed best.Aone et al. (1999) also incorporated a naive-Bayes classier, but with richer features. They describe a system called DimSum that made use of features like term frequency (tf ) and inverse document frequency (idf ) to derive signature words.Words that indicate key concepts in a document. The idf was computed from a large corpus of the same domain as the concerned documents. Statistically derived two-noun word collocations were used as units for counting, along with single words. A named-entity tagger was used and each entity was considered as a single token. They also employed some shallow discourse analysis like reference to same entities in the text, maintaining cohesion. The references were resolved at a very shallow level by linking name aliases within a document like \U.S." to \United States", or \IBM" for \International Business Machines". Synonyms and morphological variants were also merged while considering lexical terms, the former being identied by using Wordnet (Miller, 1995). The corpora used in the experiments were from newswire, some of which belonged to the TREC evaluations.2.2.2 Rich Features and Decision TreesLin and Hovy (1997) studied the importance of a single feature, sentence position. Just weighing a sentence by its position in text, which the authors term as the \position method", arises from the idea that texts generally follow a predictable discourse structure, and that the sentences of greater topic centrality tend to occur in certain speciable locations (e.g. title, abstracts, etc). However, since the discourse structure signicantly varies over domains, the position method cannot be denedas naively as in (Baxendale, 1958). The paper makes an important contribution by investigating techniques of tailoring the position method towards optimality over a genre and how it can be evaluated for eectiveness. A newswire corpus was used, the collection of Zi-Davis texts produced from the TIPSTERSee http://www.itl.nist.gov/iaui/894.02/related_projects/tipster/. program; it consists oftext about computer and related hardware, accompanied by a set of key topic wordsand a small abstract of six sentences. For each document in the corpus, the authors measured the yield of each sentence position against the topic keywords. They then ranked the sentence positions by their average yield to produce the Optimal Position Policy (OPP) for topic positions for the genre.Two kinds of evaluation were performed. Previously unseen text was used for testing whether the same procedure would work in a dierent domain. The rst evaluation showed contours exactly like the training documents. In the second eval- uation, word overlap of manual abstracts with the extracted sentences was measured. Windows in abstracts were compared with windows on the selected sentences and corresponding precision and recall values were measured. A high degree of coverage indicated the eectiveness of the position method.In later work, Lin (1999) broke away from the assumption that features are   independent of each other and tried to model the problem of sentence extraction    using decision trees, instead of a naive-Bayes classier. He examined a lot of fea- tures and their eect on sentence extraction. The data used in this work is a   publicly available collection of texts, classied into various topics, provided by the TIPSTER-SUMMACSee http://www-nlpir.nist.gov/related_projects/tipster_summac/index.html. evaluations, targeted towards information retrieval systems.The dataset contains essential text fragments (phrases, clauses, and sentences) whichmust be included in summaries to answer some TREC topics. These fragments were each evaluated by a human judge. The experiments described in the paper are withthe SUMMARIST system developed at the University of Southern California. The system extracted sentences from the documents and those were matched against human extracts, like most early work on extractive summarization.Some novel features were the query signature (normalized score given to sen- tences depending on number of query words that they contain), IR signature (the m most salient words in the corpus, similar to the signature words of (Aone et al., 1999)), numerical data (boolean value 1 given to sentences that contained a num- ber in them), proper name (boolean value 1 given to sentences that contained a proper name in them), pronoun or adjective (boolean value 1 given to sentences that contained a pronoun or adjective in them), weekday or month (similar as pre- vious feature) and quotation (similar as previous feature). It is worth noting that some features like the query signature are question-oriented because of the setting of the evaluation, unlike a generalized summarization framework.The author experimented with various baselines, like using only the positional feature, or using a simple combination of all features by adding their values. When evaluated by matching machine extracted and human extracted sentences, the deci- sion tree classier was clearly the winner for the whole dataset, but for three topics,a naive combination of features beat it. Lin conjectured that this happened because some of the features were independent of each other. Feature analysis suggested that the IR signature was a valuable feature, corroborating the early ndings of Luhn (1958).2.2.3 Hidden Markov ModelsIn contrast with previous approaches, that were mostly feature-based and non- sequential, Conroy and O'leary (2001) modeled the problem of extracting a sentencefrom a document using a hidden Markov model (HMM). The basic motivation for using a sequential model is to account for local dependencies between sentences. Only three features were used: position of the sentence in the document (built into the state structure of the HMM), number of terms in the sentence, and likeliness of the sentence terms given the document terms.no 1 no 2 no 3 noFigure 1: Markov model to extract to three summary sentences from a document (Conroy and O'leary, 2001).The HMM was structured as follows: it contained 2s + 1 states, alternating be- tween s summary states and s+1 nonsummary states. The authors allowed \hesita- tion" only in nonsummary states and \skipping next state" only in summary states.Figure 1 shows an example HMM with 7 nodes, corresponding to s = 3. Using the TREC dataset as training corpus, the authors obtained the maximum-likelihoodestimate for each transition probability, forming the transition matrix estimate M^ , whose element (i;j ) is the empirical probability of transitioning from state i to j.Associated with each state i was an output function, bi(O) = Pr(O j state i) where O is an observed vector of features. They made a simplifying assumption that the features are multivariate normal. The output function for each state was thus esti- mated by using the training data to compute the maximum likelihood estimate ofits mean and covariance matrix. They estimated 2s+1 means, but assumed that all of the output functions shared a common covariance matrix. Evaluation was doneby comparing with human generated extracts.2.2.4 Log-Linear ModelsOsborne (2002) claims that existing approaches to summarization have always as- sumed feature independence. The author used log-linear models to obviate this assumption and showed empirically that the system produced better extracts thana naive-Bayes model, with a prior appended to both models. Let c be a label, s the item we are interested in labeling, fi the i-th feature, and i the corresponding feature weight. The conditional log-linear model used by Osborne (2002) can be stated as follows: !1 XP (c j s) = Z(s) exp ifi(c;s) ; (2)iP Pwhere Z(s) = c exp( i ifi(c;s)). In this domain, there are only two possible labels: either the sentence is to be extracted or it is not. The weights were trainedby conjugate gradient descent. The authors added a non-uniform prior to the model, claiming that a log-linear model tends to reject too many sentences for inclusion ina summary. The same prior was also added to a naive-Bayes model for comparison.The classication took place as follows:!Xlabel(s) = argmax P (c) P (s;c) = argmax logP (c) + ifi(c;s) : (3)c2C c2CiThe authors optimized the prior using the f 2 score of the classier as an objective function on a part of the dataset (in the technical domain). The summaries wereevaluated using the standard f 2 score where f 2 = 2pr , where the precision and recallp+rmeasures were measured against human generated extracts. The features included word pairs (pairs of words with all words truncated to ten characters), sentence length, sentence position, and naive discourse features like inside introduction or inside conclusion. With respect to f 2 score, the log-linear model outperformed the naive-Bayes classier with the prior, exhibiting the former's eectiveness.2.2.5 Neural Networks and Third Party FeaturesIn 2001-02, DUC issued a task of creating a 100-word summary of a single news article. However, the best performing systems in the evaluations could not outper- form the baseline with statistical signicance. This extremely strong baseline has been analyzed by Nenkova (2005) and corresponds to the selection of the rst n sentences of a newswire article. This surprising result has been attributed to the journalistic convention of putting the most important part of an article in the initial paragraphs. After 2002, the task of single-document summarization for newswirewas dropped from DUC. Svore et al. (2007) propose an algorithm based on neu-ral nets and the use of third party datasets to tackle the problem of extractive summarization, outperforming the baseline with statistical signicance.The authors used a dataset containing 1365 documents gathered from CNN.com, each consisting of the title, timestamp, three or four human generated story high- lights and the article text. They considered the task of creating three machine highlights. The human generated highlights were not verbatim extractions from the article itself. The authors evaluated their system using two metrics: the rst one concatenated the three highlights produced by the system, concatenated the three human generated highlights, and compared these two blocks; the second metric con- sidered the ordering and compared the sentences on an individual level.Svore et al. (2007) trained a model from the labels and the features for each sentence of an article, that could infer the proper ranking of sentences in a test document. The ranking was accomplished using RankNet (Burges et al., 2005), a pair-based neural network algorithm designed to rank a set of inputs that uses the gradient descent method for training. For the training set, they used ROUGE-1 (Lin, 2004) to score the similarity of a human written highlight and a sentencein the document. These similarity scores were used as soft labels during training, contrasting with other approaches where sentences are \hard-labeled", as selectedor not.Some of the used features based on position or n-grams frequencies have been observed in previous work. However, the novelty of the framework lay in the useof features that derived information from query logs from Microsoft's news search engineSee http://search.live.com/news. and WikipediaSee http://en.wikipedia.org. entries. The authors conjecture that if a document sentence contained keywords used in the news search engine, or entities found in Wikipedia articles, then there is a greater chance of having that sentence in the highlight. The extracts were evaluated using ROUGE-1 and ROUGE-2, and showed statistically signicant improvements over the baseline of selecting the rst three sentences in a document.2.3 Deep Natural Language Analysis MethodsIn this subsection, we describe a set of papers that detail approaches towards single- document summarization involving complex natural language analysis techniques. None of these papers solve the problem using machine learning, but rather use a setof heuristics to create document extracts. Most of these techniques try to model the text's discourse structure.Barzilay and Elhadad (1997) describe a work that used considerable amount of       linguistic analysis for performing the task of summarization. For a better under-       standing of their method, we need to dene a lexical chain : it is a sequence of related words in a text, spanning short (adjacent words or sentences) or long distances (en-tire text). The authors' method progressed with the following steps: segmentationof the text, identication of lexical chains, and using strong lexical chains to identify the sentences worthy of extraction. They tried to reach a middle ground between (McKeown and Radev, 1995) and (Luhn, 1958) where the former relied on deepsemantic structure of the text, while the latter relied on word statistics of the doc- uments. The authors describe the notion of cohesion in text as a means of sticking together dierent parts of the text. Lexical cohesion is a notable example where semantically related words are used. For example, let us take a look at the following sentence.Example from http://www.cs.ucd.ie/staff/jcarthy/home/Lex.html.

Bibliographie :


