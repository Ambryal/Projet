Nom du Fichier :

kesslerMETICS-ICDIM2019

Titre:

A word embedding approach to explore a collection of discussions of people in psychological distress

Auteurs :

1st Remy¬¥ Kessler 2nd Nicolas Bechet¬¥ 3rd Gudrun Ledegen 4rd Frederic Pugniere-Saa` vedra Universite¬¥ Bretagne Sud Universite¬¥ Bretagne Sud Universite¬¥ Rennes II Universite¬¥ Bretagne SudCNRS 6074A CNRS 6074A PREFics, EA 4246 PREFics, EA 424656017 Vannes,France 56017 Vannes,France 5043 Rennes, France 56017 Vannes, France remy.kessler@univ-ubs.fr nicolas.bechet@irisa.fr gudrun.ledegen@univ-rennes2.fr frederic.pugniere-saavedra@univ-ubs.fr

Abstract :

Keywords‚Äîword2vec, unsupervised learning, word embedding.I. INTRODUCTIONSince the nineties, social suffering has been a theme that has received much attention from public and associative action. Among the consequences, there is an explosion of listening places or socio-technical devices of communication whose objectives consist in moderating the various forms of suffering by the liberation of the speech for a therapeutic purpose [1] [2]. As part of the METICS project, a suicide prevention association developed an application of web chat to meet this need. The web chat is an area that allows anyone to express and share with a volunteer listener their concerns and anguishes. The main specificityof this device is its anonymous nature. Protected by a pseudonym, the writers are invited to discuss with a volunteer the problematic aspects of their existence. Several thousand anonymous conversations have been gathered and form a corpus of unpublished stories about human distress. The purpose of the METICS project is to make visible the ordinary forms of suffering usually removed from common spaces and to grasp both its modes of enunciation and digital support. In this study, we want to automatically identify the reason for coming on the web chat for each participant. Indeed, even if the association provided us with the theme of all the conversations (work, loneliness, violence, racism, addictions, family, etc.), the original reason has not been preserved. In what follows, we first review some of the relatedwork in Section II. Section III presents the resources used and gives some statistics about the collection. An overview of the system and the strategy for identify the reason for coming on the web chat is given in Section IV. Section V presents the experimental protocol, an evaluation of our system and an interpretation of the final results on the collection of human distress.II. RELATED WORKSThe main characteristic of the approach presented in this paper is to only have to provide the labels of the classes to be predicted. This method does not need to have a tagged data set to predict the different classes, so it is closer to an unsupervised (clustering) or semi-supervised learning method than a supervised. The main idea of clustering is to group untagged data into a number of clusters, such that similar ex- amples are grouped together and different ones are separated. In clustering, the number of classes and the distribution of instances between classes are unknown and the goal is to find meaningful clusters.One kind of clustering methods is the partitioning-based one. The k-means algorithm [3] is one of the most popu- lar partitioning-based algorithms because it provides a good compromise between the quality of the solution obtained and its computational complexity [4]. K-means aims to find k centroids, one for each cluster, minimizing the sum of the distances of each instance of data from its respective centroid. We can cite other partitioning-based algorithms such as k- medoids or PAM (Partition Around Medoids), which is an evolution of k-means [5]. Hierarchical approaches produce clusters by recursively partitioning data backwards or upwards. For example, in a hierarchical ascending classification or CAH [6], each example from the initial dataset represents a cluster. Then, the clusters are merged, according to a similarity measure, until the desired tree structure is obtained. The result of this clustering method is called a dendrogram. Density- based methods like the EM algorithm [7] assume that the data belonging to each cluster is derived from a specific probabilitydistribution [8]. The idea is to grow a cluster as the density in the neighborhood of the cluster exceeds a predefinedthreshold. Model-based classification methods like self-organizing map - SOM [9] are focus on finding features to represent each cluster. The most used methods are decision trees and neural networks. Approaches based on semi-supervised learning such as label propagation algorithm [10] are similar to the method proposed in this paper because they consist in using a learning dataset consisting of a few labelled data points to build a model for labelling a larger number of unlabelled data. Closer to the theme of our collection, [11] and [12] use supervised approaches to automatically detect suicidal people in social networks. They extract specific features like word distribution statistics or sentiments to train different machine-learning clas- sifiers and compare performance of machine-learning models against the judgments of psychiatric trainees and mental health professionals. More recently, CLEF challenge in 2018 consists of performing a task on early risk detection of depression on texts written in Social Media1. However, these papers and this task involve tagged data sets, which is the main difference with our proposed approach (we do not have tagged data set).III. RESOURCES AND STATISTICSThe association provided a collection of conversations be- tween volunteers and callers between 2005 and 2015, which is called ‚ÄúMETICS collection‚Äù henceforth.To reduce noise in the collection, we removed all the discussions containing fewer than 15 exchanges between a caller and a person from the association, these exchanges are generally unrepresentative (connection problem, request for information, etc.). We observe particular linguistic phenomena like emoticons2, acronyms, mistakes (spelling, typography, glued words) and an explosive lexical creativity [13]. These phenomena have their origin in the mode of communication (direct or semi-direct), the speed of the composition of the message or in the technological constraints of input imposed by the material (mobile terminal, tablet, etc.). In addition, we used a subset of the collection of the French newspaper, Le Monde to validate our method on a tagged corpus. We only keep articles on television, politics, art, science or economics. Figure 1 presents some descriptive statistics of these two collections.IV. METHODOLOGYA. System OverviewFigure 2 presents an overview of the system, each step will be detailed in the rest of the section. In the first step (mod- ule ¬¨), we apply different linguistic pre-processing to each discussion. The next module (¬≠) creates a word embedding model with these discussions while the third module (¬Æ) uses this model to create specific vectors. The last module (¬Ø) performs a prediction for each discussion before separating the collection into clusters based on the predicted class.1http://early.irlab.org/2Symbols used in written messages to express emotions, e.g. smile or sadnessCollection METICS Le-Monde Total number of documents 17 594 205 661Without pre-processingTotal number of words 12 276 973 87 122 002 Total number of different words 158 361 419 579 Average words/document 698 424With pre-processingTotal number of words 4 529 793 41 425 938 Total number of different words 120 684 419 006 Average words/document 257 201Fig. 1. Statistics of both collections.Word2vec Specific model vectors creation‚ûÅ ‚ûÇClass 1 Class 2 Class n Model 1 Model 2 Model npre-processing Prediction‚ë† ‚ë£Conversationsor documents Cluster 1 Cluster 2 Cluster nFig. 2. System overviewB. Normalization and pre-processingWe first extract the textual content of each discussion. In step ¬¨, a text normalization is performed to improve the quality of the process. We remove accents, special characters such as ‚Äú-‚Äù,‚Äú/‚Äù or ‚Äú()‚Äù. Different linguistic processes are used to reduce noise in the model: we remove numbers (numeric and/or textual), special symbols and terms contained in a stop- list adapted to our problem. A lemmatization process was incorporated during the first experiments but it was inefficient considering the typographical variations described in Section III.C. word2vec modelIn the next step we build a word embedding model using word2vec [14]. We project each word of our corpus in a vector space in order to obtain a semantic representation of these. In this way, words appearing in similar contexts will have a relatively close vector representation. In addition to semantic information, one advantage of such modeling is the production of vector representations of words, depending on the context in which they are encountered. Some words close to a term t in a model learned from a corpus c1 may be very different from those from a model learned from a corpus c2. For example, we observe in figure 3 that the first eight words close to the term ‚Äúteen‚Äù vary according to the corpus used. This example also shows that the use of a generic model like Le Monde in French or Wikipedia is irrelevant in our case, since the corpuscorpuswordsMETICSteenager, young, 15years, kid, school, problem , spoiled, teen,Le-Mondesitcom, radio, compote, hearing boy, styx, scamp, rebelFig. 3. Eight words closest to the term ‚Äúteenager‚Äù according to the type of corpus in learning.of the association is noisy and contains a number of apocopes, abbreviations or acronyms. Different parameters were tested and the configuration with the best results was kept3.D. Specific vectors creation and cluster predictionsIn this step, we build vectors containing terms that are selected using the word2vec model described in step IV-C. For each theme in the collection, we build a specific linguistic model by performing a word embedding to reconstruct the linguistic context of each theme. We observe, for example, that the terms closest to the thematic ‚Äúwork‚Äù are: ‚Äúunemploy- ment‚Äù, ‚Äújob‚Äù, ‚Äústress‚Äù. Similarly, for the ‚Äúaddiction‚Äù theme, we observe the terms: ‚Äúcannabis‚Äù, ‚Äúalcoholism‚Äù, ‚Äúdrugs‚Äù and ‚Äúheroin‚Äù. We used this context subsequently to construct a vector, containing the distance distc(i) between each term i and the theme c. Each of these models is independent, so the same term can appear in several models. In this way, we observed that the word ‚Äústress‚Äù is present in the vector ‚Äúsuicide‚Äù and in that of ‚Äúwork‚Äù, however, the associated weight is different. We varied the size of these vectors between 20 and 1000 and the best results were obtained with a size of 400. In the last step ¬Ø, the system computes an Sc score for each discussion and for each cluster according to each linguistic model such as:XnSc(d) = tf (i) distc(i) (1)i=1with i the considered term, tf (i) frequency of i in the collection, and distc(i) is the distance between the term i and the thematic c. In the end, the class with the highest score is chosen.V. EXPERIMENTS AND RESULTSA. Experimental protocolTo evaluate the quality of the obtained clusters, we used a subset of the texts of the Le-Monde newspaper, described in Section III, each article having a label according to the theme. For these experiments, we configuredthe specificvectors (SV) approach with the optimal parameters, as defined in Sections IV-C and IV-D. We also tested the specific vectors without weighting to test the particular influence of this parameter. To highlight the difficulty of the task, we compare our system with a baseline which consists in a random draw, and with3The best results were obtained with the following parameter values: vector size: 700, sliding window size: 5, minimum frequency: 10, vectorization method: skip grams, and a soft hierarchical max function for the model learning.the k-means algorithm [3], commonly used in the literature, as mentioned in Section II. To feed the k-means algorithm, we transformed our initial collection into a bag of words matrix [15] where each conversation is described by the frequency of its words. Each of the experiments was evaluated using the classic measures of Precision, Recall and F-measure, averaged over all classes (with beta = 1 in order not to privilege precision or recall [16]). Since the k-means algorithm does not associate a tag with the final clusters, we have exhaustively calculated the set of solutions to keep only the one yielding the highest F-score.B. Results Prec.RecallF-scoreWithout pre-processing