
Evaluation Only. Created with Aspose.Words. Copyright 2003-2023 Aspose Pty Ltd.
Periods, Capitalized Words, etc.
Andrei Mikheev ∗
University of Edinburgh
In this article we present an approach for tackling three important aspects of text normaliza- tion: sentence boundary disambiguation, disambiguation of capitalized words in positions where capitalization is expected, and identi cation of abbreviations. As opposed to the two dominant techniques of computing statistics or writing specialized grammars, our document-centered ap- proachworksbyconsideringsuggestivelocalcontextsandrepetitionsofindividualwordswithin adocument.Thisapproachprovedtoberobusttodomainshiftsandnewlexicaandproducedper- formance on the level with the highest reported results. When incorporated into a part-of-speech tagger, it helped reduce the error rate signi cantly on capitalized words and sentence boundaries. We also investigated the portability to other languages and obtained encouraging results.
1. Introduction
Disambiguation of sentence boundaries and normalization of capitalized words, as well as identi cation of abbreviations, however small in comparison to other tasks of text processing, are of primary importance in the developing of practical text- processing applications. These tasks are usually performed before actual intelligent text processing starts, and errors made at this stage are very likely to cause more errors at later stages and are therefore very dangerous.
Disambiguation of capitalized words in mixed-case texts has received little atten- tion in the natural language processing and information retrieval communities, but in fact it plays an important role in many tasks. In mixed-case texts capitalized words usually denote proper names (names of organizations, locations, people, artifacts, etc.), but there are special positions in the text where capitalization is expected. Such manda- tory positions include the rst word in a sentence, words in titles with all signi cant words capitalized or table entries, a capitalized word after a colon or open quote, and the rst word in a list entry, among others. Capitalized words in these and some other positions present a case of ambiguity: they can stand for proper names, as in White later said ... , or they can be just capitalized common words, as in White elephants are ... . The disambiguation of capitalized words in ambiguous positions leads to the identi cation of proper names (or their derivatives), and in this article we will use these two terms and the term case normalization interchangeably.
Church (1995, p. 294) studied, among other simple text normalization techniques,   the effect of case normalization for different words and showed that sometimes case      variants refer to the same thing (hurricane and Hurricane), sometimes they refer to different things (continental and Continental) and sometimes they don t refer to much
of anything (e.g., anytime and Anytime).  Obviously these differences arise because some capitalized words stand for proper names (such as Continental, the name of an airline) and some do not.
∗Institute for Communicating and Collaborative Systems, Division of Informatics, 2 Buccleuch Place,
Edinburgh EH8 9LW, UK. E-mail: mikheev@cogsci.ed.ac.uk
 c 2002 Association for Computational Linguistics

Created with an evaluation copy of Aspose.Words. To discover the full versions of our APIs please visit: https://products.aspose.com/words/
Mikheev Periods, Capitalized Words, etc.
Proper names are the main concern of the named-entity recognition subtask (Chin- chor 1998) of information extraction. The main objective of this subtask is the identi-  cation of proper names and also their classi cation into semantic categories (person, organization, location, etc.). In this article we are concerned only with the identi cation of proper names.
 There the disambiguation of the rst word in a sentence (and in other ambiguous positions) is one of the central problems: about 20% of named entities occur in ambiguous positions. For instance, the word Black in the sentence- initial position can stand for a person s surname but can also refer to the color. Even in multiword capitalized phrases, the rst word can belong to the rest of the phrase or can be just an external modi er. In the sentence Daily, Mason and Partners lost their court case, it is clear that Daily, Mason and Partners is the name of a company. In the sentence Unfortunately,Mason and Partners losttheircourt case, the name of the company does not include the word Unfortunately, but the word Daily is just as common a word as Unfortunately.
Identi cation of proper names is also important in machine translation, because usually proper names are transliterated (i.e., phonetically translated) rather than prop- erly (semantically) translated. In con dential texts, such as medical records, proper names must be identi ed and removed before making such texts available to people unauthorized to have access to personally identi able information. And in general, most tasks that involve text analysis will bene t from the robust disambiguation of capitalized words into proper names and common words.
Another important task of text normalization is sentence boundary disambigua- tion (SBD) or sentence splitting. Segmenting text into sentences is an important aspect in developing many applications: syntactic parsing, information extraction, machine translation, question answering, text alignment, document summarization, etc. Sen- tence splitting in most cases is a simple matter: a period, an exclamation mark, or a question mark usually signals a sentence boundary. In certain cases, however, a period denotes a decimal point or is a part of an abbreviation, and thus it does not necessarily signal a sentence boundary. Furthermore, an abbreviation itself can be the last token in a sentence in which case its period acts at the same time as part of this abbreviation and as the end-of-sentence indicator (fullstop). A detailed introduction to the SBD problem can be found in Palmer and Hearst (1997).
The disambiguation of capitalized words and sentence boundaries presents a chicken-and-egg problem. If we know that a capitalized word that follows a period is a common word, we can safely assign such period as sentence terminal. On the other hand, if we know that a period is not sentence terminal, then we can conclude that the following capitalized word is a proper name.
Another frequent source of ambiguity in end-of-sentence marking is introduced by abbreviations: if we know that the word that precedes a period is not an abbreviation, then almost certainly this period denotes a sentence boundary. If, however, this word is an abbreviation, then it is not that easy to make a clear decision. This problem is exacerbated by the fact that abbreviations do not form a closed set; that is, one can- not list all possible abbreviations. Moreover, abbreviations can coincide with regular words; for example, in  can denote an abbreviation for inches,  no  can denote an abbreviation for number,  and bus  can denote an abbreviation for business. 
In this article we present a method that tackles sentence boundaries, capitalized words, and abbreviations in a uniform way through a document-centered approach. As opposed to the two dominant techniques of computing statistics about the words that surround potential sentence boundaries or writing specialized grammars, our ap- proach disambiguates capitalized words and abbreviations by considering suggestive local contexts and repetitions of individual words within a document. It then applies this information to identify sentence boundaries using a small set of rules.
2. Performance Measure, Corpora for Evaluation, and Intended Markup
A standard practice for measuring the performance of a system for the class of tasks with which we are concerned in this article is to calculate its error rate:
incorrectly assigned error rate =
all assigned by system
This single measure gives enough information, provided that the system does not leave unassigned word tokens that it is intended to handle. Obviously, we want the system to handle all cases as accurately as possible. Sometimes, however, it is bene cial to assign only cases in which the system is con dent enough, leaving the rest to be handled by other methods. In this case apart from the error rate (which corresponds to precision or accuracy as 1 − error rate) we also measure the system s coverage or recall
correctly assigned
coverage=
all to be assigned
2.1 Corpora for Evaluation
There are two corpora normally used for evaluation in a number of text-processing tasks: the Brown corpus (Francis and Kucera 1982) and the Wall Street Journal (WSJ) corpus, both part of the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993).
The Brown corpus represents general English. It contains over one million word tokens and is composed from 15 subcorpora that belong to different genres and domains, ranging from news wire texts and scienti c papers to ction and transcribed speech. The Brown corpus is rich in out-of-vocabulary (unknown) words, spelling errors, and ungrammatical sentences with complex internal structure. Altogether there are about 500 documents in the Brown corpus, with an average length of 2,300 word tokens.
The WSJ corpus represents journalistic news wire style. Its size is also over a million word tokens, and the documents it contains are rich in abbreviations and proper names, but they are much shorter than those in the Brown corpus. Altogether there are about 2,500 documents in the WSJ corpus, with an average length of about 500 word tokens.
Documents in the Penn Treebank are segmented into paragraphs and sentences. Sentences are further segmented into word tokens annotated with part-of-speech (POS) information. POS information can be used to distinguish between proper names and common words. We considered proper nouns (NNP), plural proper nouns (NNPS), and proper adjectives These are adjectives derived from proper nouns (e.g. American ).
 (JJP) to signal proper names, and all other categories were consid- ered to signal common words or punctuation. Since proper adjectives are not included in the Penn Treebank tag set, we had to identify and retag them ourselves with the help of a gazetteer.
Abbreviations in the Penn Treebank are tokenized together with their trailing pe- riods, whereas fullstops and other sentence boundary punctuation are tokenized as separate tokens. This gives all necessary information for the evaluation in all our three tasks: the sentence boundary disambiguation task, the capitalized word disambigua- tion task, and the abbreviation identi cation task.
2.2 Tokenization Convention and Corpora Markup
For easier handling of potential sentence boundary punctuation, we developed a new tokenization convention for periods. In the traditional Penn Treebank schema, abbrevi- ations are tokenized together with their trailing periods, and thus stand-alone periods unambiguously signal the end of a sentence. We decided to treat periods and all other potential sentence termination punctuation as  rst-class citizens  and adopted a con- vention always to tokenize a period (and other punctuation) as a separate token when it is followed by a white space, line break, or punctuation. In the original Penn Tree- bank format, periods are unambiguous, whereas in our new convention they can take on one of the three tags: fullstop (.), part of abbreviation (A) or both (*).
To generate the new format from the Penn Treebank, we had to split nal periods from abbreviations, mark them as separate tokens and assign them with Aor * tags according to whether or not the abbreviation was the last token in a sentence. We applied a similar tokenization convention to the case in which several (usually three)
periods signal ellipsis in a sentence. Again, sometimes such constructions occur within a sentence and sometimes at a sentence break. We decided to treat such constructions similarly to abbreviations, tokenize all periods but the last together in a single token,
and tokenize the last period separately and tag it with Aor * according to whether or not the ellipsis was the last token in a sentence. We treated periods in numbers
(e.g., 14.534) or inside acronyms (e.g., Y.M.C.A.) as part of tokens rather than separate periods.
In all our experiments we treated embedded sentence boundaries in the same way as normal sentence boundaries. An embedded sentence boundary occurs when there is a sentence inside a sentence. This can be a quoted direct-speech subsentence inside a sentence, a subsentence embedded in brackets, etc. We considered closing punctuation of such sentences equal to closing punctuation of normal sentences.
We also specially marked word tokens in positions where they were ambiguously capitalized if such word tokens occurred in one of the following contexts:
• the rst token in a sentence
• following a separately tokenized period, question mark, exclamation mark, semicolon, colon, opening quote, closing quote, opening bracket, or closed bracket
• occurring in a sentence with all words capitalized
All described transformations were performed automatically by applying a simple Perl script. We found quite a few infelicities in the original tokenization and tagging, however, which we had to correct by hand. We also converted both our corpora from their original Penn Treebank format into an XML format where each word token is represented as an XML element (W) with the attribute C holding its POS information and attribute A set to Y for ambiguously capitalized words. An example of such a markup is displayed in Figure 1.
3. Our Approach to Sentence Boundary Disambiguation
If we had at our disposal entirely correct information on whether or not each word preceding a period was an abbreviation and whether or not each capitalized word
...<W C=RB>soon</W><W C=’.’>.</W> <W A=Y C=NNP>Mr</W><W C=A>.</W>...
...<W C=VBN>said</W> <W C=NNP>Mr</W><W C=A>.</W>
<W A=Y C=NNP>Brown</W>...
...<W C=’,’>,</W> <W C=NNP>Tex</W><W C=’*’>.</W>
<W A=Y C=JJP>American</W>...
Figure 1
Example of the new tokenization and markup generated from the Penn Treebank format. Tokens are represented as XML elements W, where the attribute Cholds POS information. Proper names are tagged as NNP, NNPSand JJP. Periods are tagged as . (fullstop), A(part of abbreviation), * (a fullstop and part of abbreviation at the same time). Ambiguously
capitalized words are marked with A = Y.
that follows a period was a proper name, we could apply a very simple set of rules to disambiguate sentence boundaries:
• If a period follows a nonabbreviation, it is sentence terminal ( . ).
• If a period follows an abbreviation and is the last token in a text passage (paragraph, document, etc.), it is sentence terminal and part of the abbreviation (*).
• If a period follows an abbreviation and is not followed by a capitalized word, it is part of the abbreviation and is not sentence terminal (A).
• If a period follows an abbreviation and is followed by a capitalized word that is not a proper name, it is sentence terminal and part of the abbreviation (*).
It is a trivial matter to extend these rules to allow for brackets and quotation marks between the period and the following word. To handle other sentence termination punctuation such as question and exclamation marks and semicolons, this rule set also needs to include corresponding rules. The entire rule set for sentence boundary disambiguation that was used in our experiments is listed in Appendix A.
3.1 Ideal Case: Upper Bound for Our SBD Approach
The estimates from the Brown corpus and the WSJ corpus (section 3) show that the application of the SBD rule set described above together with the information on abbreviations and proper names marked up in the corpora produces very accurate results (error rate less than 0.0001%), but it leaves unassigned the outcome of the case in which an abbreviation is followed by a proper name. This is a truly ambiguous case, and to deal with this situation in general, one should encode detailed information about the words participating in such contexts. For instance, honori c abbreviations such as Mr. or Dr. when followed by a proper name almost certainly do not end a sentence, whereas the abbreviations of U.S. states such as Mo., Cal., and Ore., when followed directly by a proper name, most likely end a sentence. Obviously encoding this kind of information into the system requires detailed analysis of the domain lexica, is not robust to unseen abbreviations, and is labor intensive.
To make our method robust to unseen words, we opted for a crude but simple solution. If such ambiguous cases are always resolved as not sentence boundary  (A), this produces, by our measure, an error rate of less than 3%. Estimates from the Brown
Table 1
Estimates of the upper and lower bound error rates on the SBD task for our method. Three estimated categories are sentence boundaries, ambiguously capitalized words, and abbreviations.
Brown Corpus WSJ Corpus
SBD Amb. Cap. Abbreviations SBD Amb. Cap. Abbreviations Number of 59,539 58,957 4,657 53,043 54,537 16,317
resolved instances
A Upper Bound: 0.01% 0.0% 0.0% 0.13% 0.0% 0.0%
All correct proper
names
All correct abbrs.
B Lower Bound: 2.00% 7.4% 10.8% 4.10% 15.0% 9.6%
Lookup proper
names
Guessed abbrs.
C Lookup proper 1.20% 7.4% 0.0% 2.34% 15.0% 0.0%
names
All correct abbrs.
D All correct proper 0.45% 0.0% 10.8% 1.96% 0.0% 9.6%
names
Guessed abbrs.
corpus and the WSJ corpus showed that such ambiguous cases constitute only 5—7% of all potential sentence boundaries. This translates into a relatively small impact of the crude strategy on the overall error rate on sentence boundaries. This impact was measured at 0.01% on the Brown corpus and at 0.13% on the WSJ corpus, as presented in row A of Table 1. Although this overly simplistic strategy extracts a small penalty from the performance, we decided to use it because it is very general and independent of domain-speci c knowledge.
The SBD handling strategy described above is simple, robust, and well perform- ing, but it relies on the assumption that we have entirely correct information about abbreviations and proper names, as can be seen in row A of the table. The main dif-  culty is that when dealing with real-world texts, we have to identify abbreviations and proper names ourselves. Thus estimates based on the application of our method when using 100% correctly disambiguated capitalized words and abbreviations can be considered as the upper bound for the SBD approach, that is, the top performance we can achieve.
3.2 Worst Case: Lower Bound for Our SBD Approach
We can also estimate the lower bound for this approach applying very simple strategies to the identi cation of proper names and abbreviations.
The simplest strategy for deciding whether or not a capitalized word in an ambigu- ous position is a proper name is to apply a lexical-lookup strategy (possibly enhanced with a morphological word guesser, e.g., Mikheev [1997]). Using this strategy, words not listed as known common words for a language are usually marked as proper names. The application of this strategy produced a 7.4% error rate on the Brown corpus and a 15% error rate on the WSJ corpus. The difference in error rates can be explained by the observation that the WSJ corpus contains a higher percentage of orga- nization names and person names, which often coincide with common English words, and it contains more words in titles with all important words capitalized, which we also consider as ambiguously capitalized.
The simplest strategy for deciding whether a word that is followed by a period is an abbreviation or a regular word is to apply well-known heuristics based on the observation that single-word abbreviations are short and normally do not include vowels (Mr., Dr., kg.). Thus a word without vowels can be guessed to be an abbreviation unless it is written in all capital letters and can stand for an acronym or a proper name (e.g., BBC). A span of single letters separated by periods forms an abbreviation too (e.g., Y.M.C.A.). A single letter followed by a period is also a very likely abbreviation. There is also an additional heuristic that classi es as abbreviations short words (with length less than ve characters) that are followed by a period and then by a comma, a lower-cased word, or a number. All other words are considered to be nonabbreviations.
These heuristics are reasonably accurate. On the WSJ corpus they misrecognized as abbreviations only 0.2% of tokens. On the Brown corpus the misrecognition rate was signi cantly higher: 1.6%. The major source for these errors were single letters that stand for mathematical symbols in the scienti c subcorpora of the Brown Corpus (e.g., point T or triangle F). The major shortcoming of these abbreviation-guessing heuristics, however, comes from the fact that they failed to identify about 9.5% of abbreviations. This brings the overall error rate of the abbreviation-guessing heuristics to about 10%.
Combining the information produced by the lexical-lookup approach to proper name identi cation with the abbreviation-guessing heuristics feeding the SBD rule set gave us a 2.0% error rate on the Brown corpus and 4.1% on the WSJ corpus on the SBD task. This can be interpreted as the lower bound to our SBD approach. Here we see how errors in the identi cation of proper names and abbreviations propagated themselves into errors on sentence boundaries. Row B of Table 1 displays a summary for the lower-bound results.
3.3 Major Findings
We also measured the importance of each of the two knowledge sources (abbreviations and proper names) separately. First, we applied the SBD rule set when all abbreviations were correctly identi ed (using the information presented in the corpus) but applying the lexical lookup strategy to proper-name identi cation (row C of Table 1). Then, we applied the SBD rule set when all proper names were correctly identi ed (using the information presented in the corpus) but applying the guessing heuristics to handle abbreviations (row D of the table). In general, when a knowledge source returned 100% accurate information this signi cantly improved performance on the SBD task measured against the lower-bound error rate. We also see that proper names have a higher impact on the SBD task than abbreviations.
Since the upper bound of our SBD approach is high and the lower bound is far from being acceptable, our main strategy for sentence boundary disambiguation will be to invest in the disambiguation of capitalized words and abbreviations that then feed our SBD rule set.
4. Document-Centered Approach to Proper Name and Abbreviation Handling
As we discussed above, virtually any common word can potentially act as a proper name or part of a multiword proper name. The same applies to abbreviations: there is no xed list of abbreviations, and almost any short word can be used as an abbrevia- tion. Fortunately, there is a mitigating factor too: important words are typically used in a document more than once and in different contexts. Some of these contexts create ambiguity, but some do not. Furthermore, ambiguous words and phrases are usually unambiguously introduced at least once in the text unless they are part of common knowledge presupposed to be possessed by the readers.
This observation can be applied to a broader class of tasks. For example, people      are often referred to by their surnames (e.g., Black) but are usually introduced at least once in the text either with their rst name (John Black) or with their title/profession
af liation (Mr. Black, President Bush), and it is only when their names are common knowledge that they do not need an introduction (e.g., Castro, Gorbachev). Thus our suggestion is to lookattheunambiguoususagesofthewordsinquestionintheentiredocument.
In the case of proper name identi cation, we are not concerned with the semantic class of a name (e.g., whether it is a person s name or a location), but rather we simply want to distinguish whether a capitalized word in a particular occurrence acts as a proper name (or part of a multiword proper name). If we restrict our scope to a single sentence, we might nd that there is just not enough information to make a reliable decision. For instance, Riders in the sentence Ridersrodealloverthegreenis equally likely to be a proper noun, a plural proper noun, or a plural common noun. But if in the same text we nd JohnRiders, this sharply increases the likelihood that the proper noun interpretation is the correct one, and conversely if we nd many riders, this suggests the plural-noun interpretation.
The above reasoning can be summarized as follows: if we detect that a word is used capitalized in an unambiguous context, this increases the chances that this word acts as a proper name in ambiguous positions in the same document. And conversely if a word is seen only lower-cased, this increases the chances that it should be treated as a common word even when used capitalized in ambiguous positions in the same document. (This, of course, is only a general principle and will be further elaborated elsewhere in the article.)
The same logic applies to abbreviations. Although a short word followed by a period is a potential abbreviation, the same word occurring in the same document in a different context can be unambiguously classi ed as a regular word if it is used without a trailing period, or it can be unambiguously classi ed as an abbreviation if it is used with a trailing period and is followed by a lower-cased word or a comma. This information gives us a better chance of assigning these potential abbreviations correctly in nonobvious contexts.
We call such style of processing a document-centered approach (DCA), since in- formation for the disambiguation of an individual word token is derived from the entire document rather than from its immediate local context. Essentially the system collects suggestive instances of usage for target words from each document under processing and applies this information on the y to the processing of the document, in a manner similar to instance-based learning. This differentiates DCA from the tradi- tional corpus-based approach, in which learning is applied prior to processing, which is usually performed with supervision over multiple documents of the training corpus.
5. Building Support Resources
Our method requires only four word lists. Each list is a collection of words that belong to a single type, but at the same time, a word can belong to multiple lists. Since we have four lists, we have four types:
• common word (as opposed to proper name)
• common word that is a frequent sentence starter
• frequent proper name
• abbreviation (as opposed to regular word)
These four lists can be acquired completely automatically from raw (unlabeled) texts. For the development of these lists we used a collection of texts of about 300,000 words derived from the New York Times (NYT) corpus that was supplied as training data for
the 7th Message Understanding Conference (MUC-7) (Chinchor 1998). We used these texts because the approach described in this article was initially designed to be part of a named-entity recognition system (Mikheev, Grover, and Moens 1998) developed for MUC-7. Although the corpus size of 300,000 words can be seen as large, the fact that this corpus does not have to be annotated in any way and that a corpus of similar size can be easily collected from on-line sources (including the Internet) makes this resource cheap to obtain.
The rst list on which our method relies is a list of common words. This list includes common words for a given language, but no supplementary information such as POS or morphological information is required to be present in this list. A variety of such lists for many languages are already available (e.g., Burnage 1990). Words in such lists are usually supplemented with morphological and POS information (which is not required by our method). We do not have to rely on pre-existing resources, however. A list of common words can be easily obtained automatically from a raw (unannotated in any way) text collection by simply collecting and counting lower- cased words in it. We generated such list from the NYT collection. To account for potential spelling and capitalization errors, we included in the list of common words only words that occurred lower-cased at least three times in the NYT texts. The list of common words that we developed from the NYT collection contained about 15,000 English words.
The second list on which our method relies is a frequent-starters list, a list of common words most frequently used in sentence-starting positions. This list can also be obtained completely automatically from an unannotated corpus by applying the lexical-lookup strategy. As discussed in Section 3.2, this strategy performs with a 7—15% error rate. We applied the list of common words over the NYT text collec-
tion to tag capitalized words in sentence-starting positions as common words and as proper names: if a capitalized word was found in the list of common words, it was tagged as a common word: otherwise it was tagged as a proper name. Of course, such tagging was far from perfect, but it was good enough for our purposes. We included in the frequent-starters list only the 200 most frequent sentence-starting common words. This was more than a safe threshold to ensure that no wrongly tagged words were added to this list. As one might predict, the most frequent sentence-starting common word was The. This list also included some adverbs, such as However, Suddenly, and Once; some prepositions, such as In, To, and By; and even a few verbs: Let, Have, Do, etc.
The third list on which our method relies is a list of single-word proper names that coincide with common words. For instance, the word Japanis much more likely to be used as a proper name (name of a country) rather than a verb, and therefore it needs to
be included in this list. We included in the proper name list 200 words that were most frequently seen in the NYT text collection as single capitalized words in unambiguous positions and that at the same time were present in the list of common words. For instance, the word The can frequently be seen capitalized in unambiguous positions,
but it is always followed by another capitalized word, so we do not count it as a candidate. On the other hand the word China is often seen capitalized in unambiguous positions where it is not preceded or followed by other capitalized words. Since china
Table 2
Error rates for different combinations of the abbreviation identi cation methods, including combinations of guessing heuristics (GH), lexical lookup (LL), and the document-centered approach (DCA).
Abbreviation Identi cation Method WSJ Brown
A GH 9.6% 10.8% B LL 12.6% 11.9% C GH + LL 1.2% 2.1% D GH + DCA 6.6% 8.9% E GH + DCA + LL 0.8% 1.2%
is also listed among common words and is much less frequently used in this way, we include it in the proper name list.
The fourth list on which our method relies is a list of known abbreviations. Again, we induced this list completely automatically from an unannotated corpus. We applied the abbreviation-guessing heuristics described in Section 6 to our NYT text collection and then extracted the 270 most frequent abbreviations: all abbrevi- ations that appeared ve times or more. This list included honori c abbreviations (Mr, Dr), corporate designators (Ltd, Co), month name abbreviations (Jan, Feb), ab- breviations of names of U.S. states (Ala, Cal), measure unit abbreviations (ft, kg), etc. Although we described these abbreviations in groups, this information was not en- coded in the list; the only information this list provides is that a word is a known abbreviation.
Among these four lists the rst three re ect general language regularities and usually do not require modi cation for handling texts from a new domain. The ab- breviation list, however, is much more domain dependent and for better performance needs to be reinduced for a new domain. Since the compilation of all four lists does not require data preannotated in any way, it is very easy to specialize the above-described lists to a particular domain: we can simply rebuild the lists using a domain-speci c corpus. This process is completely automatic and does not require any human labor apart from collecting a raw domain-speci c corpus. Since all cutoff thresholds that we applied here were chosen by intuition, however, different domains might require some new settings.
6. Recognizing Abbreviations
The answer to the question of whether or not a particular word token is an abbreviation or a regular word largely solves the sentence boundary problem. In the Brown corpus 92% of potential sentence boundaries come after regular words. The WSJ corpus is richer with abbreviations, and only 83% of sentences in that corpus end with a regular word followed by a period. In Section 3 we described the heuristics for abbreviation guessing and pointed out that although these heuristics are reasonably accurate, they fail to identify about 9.5% of abbreviations. Since unidenti ed abbreviations are then treated as regular words, the overall error rate of the guessing heuristics was measured at about 10% (row A of Table 2). Thus, to improve this error rate, we need rst of all to improve the coverage of the abbreviation-handling strategy.
A standard way to do this is to use the guessing heuristics in conjunction with a list of known abbreviations. We decided to use the list of 270 abbreviations described in Section 5. First we applied only the lexical-lookup strategy to our two corpora (i.e., only when a token was found in the list of 270 known abbreviations was it marked as an abbreviation). This gave us an unexpectedly high error rate of about 12%, as displayed in row B of Table 2. When we investigated the reason for the high error rate, we found that the majority of single letters and spans of single letters sepa- rated by periods (e.g. Y.M.C.A.) found in the Brown corpus and the WSJ corpus were not present in our abbreviation list and therefore were not recognized as abbrevia- tions.
Such cases, however, are handled well by the abbreviation-guessing heuristics. When we applied the abbreviation list together with the abbreviation-guessing heuris- tics (row C of Table 2), this gave us a very strong performance on the WSJ corpus: an error rate of 1.2%. On the Brown corpus, the error rate was higher: 2.1%. This can be explained by the fact that we collected our abbreviation list from a corpus of news articles that is not too dissimilar to the texts in the WSJ corpus and thus, this list con- tained many abbreviations found in that corpus. The Brown corpus, in contrast, ranges across several different domains and sublanguages, which makes it more dif cult to compile a list from a single corpus to cover it.
6.1 Unigram DCA
The abbreviation-guessing heuristics supplemented with a list of abbreviations are accurate, but they still can miss some abbreviations. For instance, if an abbreviation like sec or Okla. is followed by a capitalized word and is not listed in the list of abbreviations, the guessing heuristics will not uncover them. We also would like to boost the abbreviation handling with a domain-independent method that enables the system to function even when the abbreviation list is not much of a help. Thus, in addition to the list of known abbreviations and the guessing heuristics, we decided to apply the DCA as described below.
Each word of length four characters or less that is followed by a period is treated as a potential abbreviation. First, the system collects unigrams of potential abbreviations in unambiguous contexts from the document under processing. If a potential abbre- viation is used elsewhere in the document without a trailing period, we can conclude that it in fact is not an abbreviation but rather a regular word (nonabbreviation). To decide whether a potential abbreviation is really an abbreviation, we look for contexts in which it is followed by a period and then by a lower-cased word, a number, or a comma.
For instance, the word Kong followed by a period and then by a capitalized word cannot be safely classi ed as a regular word (nonabbreviation), and therefore it is a potential abbreviation. But if in the same document we detect a context lived in Hong Kong in 1993, this indicates that Kong in this document is normally written without
a trailing period and hence is not an abbreviation. Having established that, we can apply this information to nonevident contexts and classify Kong as a regular word throughout the document. However, if we detect a context such as Kong., said, this indicates that in this document, Kong is normally written with a trailing period and hence is an abbreviation. This gives us grounds for classifying Kong as an abbreviation in all its occurrences within the same document.
6.2 Bigram DCA
The DCA relies on the assumption that there is a consistency in writing within the same document. Different authors can write Mr or Dr with or without a trailing period, but we assume that the same author (the author of a particular document) writes
it in the same way consistently. A situation can arise, however, in which the same potential abbreviation is used as a regular word and as an abbreviation within the same document. This is usually the case when an abbreviation coincides with a regular word, for example Sun. (meaning Sunday) and Sun (the name of a newspaper). To tackle this problem, the system can collect from a document not only unigrams of potential abbreviations in unambiguous contexts but also their bigrams with the preceding word. Of course, as in the case with unigrams, the bigrams are collected on the y
and completely automatically.
For instance, if the system nds a context vitamin C is, it stores the bigram vita- min C and the unigram C with the information that C is a regular word. If in the same document the system also detects a context John C. later said, it stores the bi- gram John C and the unigram C with the information that C is an abbreviation. Here we have con icting information for the word C: it was detected to act as a regu- lar word and as an abbreviation within the same document, so there is not enough information to resolve ambiguous cases purely using the unigram. Some cases, how-
ever, can still be resolved on the basis of the bigrams. The system will assign C as a regular word (nonabbreviation) in an ambiguous context such as vitamin C. Research because of the stored vitamin C bigram. Obviously from such a short context, it is
dif cult even for a human to make a con dent decision, but the evidence gathered from the entire document can in uence this decision with a high degree of con - dence.
6.3 Resulting Approach
When neither unigrams nor bigrams can help to resolve an ambiguous context for a potential abbreviation, the system decides in favor of the more frequent category for that abbreviation. If the word In was detected to act as a regular word (preposition) ve
times in the current document and two times as abbreviation (for the state Indiana), in a context in which neither of the bigrams collected from the document can be applied, In is assigned as a regular word (nonabbreviation). The last-resort strategy is to assign all nonresolved cases as nonabbreviations.
Row D of Table 2 shows the results when we applied the abbreviation-guessing heuristics together with the DCA. On the WSJ corpus, the DCA reduced the error rate of the guessing heuristics alone (row A) by about 30%; on the Brown corpus its impact was somewhat smaller, about 18%. This can be explained by the fact that abbreviations in the WSJ corpus have a much higher repetition rate, which is very important for the DCA.
We also applied the DCA together with the lexical lookup and the guessing heuris- tics. This reduced the error rate on abbreviation identi cation by about 30% in com- parison with the list and guessing heuristics con guration, as can be seen in row E of Table 2.
7. Disambiguating Capitalized Words
The second key task of our approach is the disambiguation of capitalized words that follow a potential sentence boundary punctuation sign. Apart from being an important component in the task of text normalization, information about whether or not a capitalized word that follows a period is a common word is crucial for the SBD task, as we showed in Section 3. We tackle capitalized words in a similar fashion as we tackled the abbreviations: through a document-centered approach that analyzes on the y the distribution of ambiguously capitalized words in the entire document. This is implemented as a cascade of simple strategies, which were brie y described in Mikheev (1999).
7.1 The Sequence Strategy
The rst DCA strategy for the disambiguation of ambiguous capitalized words is to explore sequences of words extracted from contexts in which the same words are used unambiguously with respect to their capitalization. We call this the sequence strategy. The rationale behind this strategy is that if there is a phrase of two or more capitalized words starting from an unambiguous position (e.g., following a lower-cased word), the system can be reasonably con dent that even when the same phrase starts from an unreliable position (e.g., after a period), all its words still have to be grouped together and hence are proper nouns. Moreover, this applies not just to the exact replication of the capitalized phrase, but to any partial ordering of its words of size two characters or more preserving their sequence.
For instance, if a phrase Rocket Systems Development Co. is found in a document starting from an unambiguous position (e.g., after a lower-cased word, a number, or a comma), the system collects it and also generates its partial-order subphrases: Rocket Systems, Rocket Systems Co., Rocket Co., Systems Development, etc. If then in the same document Rocket Systems is found in an ambiguous position (e.g., after a period), the system will assign the word Rocket as a proper noun because it is part of a multiword proper name that was seen in the unambiguous context.
A span of capitalized words can also internally include alpha-numerals, abbrevia- tions with internal periods, symbols, and lower-cased words of length three characters or shorter. This enables the system to capture phrases like A & M and The Phantom of the Opera. Partial orders from such phrases are generated in a similar way, but with the restriction that every generated subphrase should start and end with a capitalized word.
The sequence strategy can also be applied to disambiguate common words. Since in the case of common words the system cannot determine boundaries of a phrase, only bigrams of the lower-cased words with their following words are collected from the document. For instance, from a context continental suppliers of Mercury, the sys- tem collects three bigrams: continental suppliers, suppliers of, and of Mercury. When the system encounters the phrase Continental suppliers after a period, it can now use the information that in the previously stored bigram continental suppliers, the word token continental was written lower-cased and therefore was unambiguously used as a com- mon word. On this basis the system can assign the ambiguous capitalized word token Continental as a common word.
Row A of Table 3 displays the results obtained in the application of the sequence strategy to the Brown corpus and the WSJ corpus. The sequence strategy is extremely useful when we are dealing with names of organizations, since many of them are multiword phrases composed of common words. For instance, the words Rocket and Insurance can be used both as proper names and common words within the same document. The sequence strategy maintains contexts of the usages of such words within the same document, and thus it can disambiguate such usages in the ambiguous positions matching surrounding words. And indeed, the error rate of this strategy when applied to proper names was below 1%, with coverage of about 9—12%.
For tagging common words the sequence strategy was also very accurate (error rate less than 0.3%), covering 17% of ambiguous capitalized common words on the WSJ corpus and 25% on the Brown corpus. The higher coverage on the Brown corpus can be explained by the fact that the documents in that corpus are in general longer than those in the WSJ corpus, which enables more word bigrams to be collected from a document.
Dual application of the sequence strategy contributes to its robustness against po- tential capitalization errors in the document. The negative evidence (not proper name)
Table 3
First part: Error rates of different individual strategies for capitalized-word disambiguation. Second part: Error rates of the overall cascading application of the individual strategies.
Strategy Word Class Error Rate Coverage
WSJ Brown WSJ Brown

Created with an evaluation copy of Aspose.Words. To discover the full versions of our APIs please visit: https://products.aspose.com/words/
301
Mikheev Periods, Capitalized Words, etc.
A Sequence strategy
Sequence strategy
B Frequent-list lookup strategy Frequent-list lookup strategy
C Single-word assignment strategy
Single-word assignment strategy D Cascading DCA
E Cascading DCA
and lexical lookup
Proper Names Common Words
Proper Names Common Words Proper Names Common Words
Proper/Common Proper/Common
0.12% 0.97% 0.28% 0.21%
0.49% 0.16% 0.21% 0.14% 3.18% 1.96% 6.51% 2.87%
1.10% 0.76% 4.88% 2.83%
12.6% 8.82% 17.68% 26.5% 2.62% 6.54%
64.62% 61.20% 18.77% 34.13%
3.07% 4.78% 84.12% 91.76%
100.0% 100.0%

Created with an evaluation copy of Aspose.Words. To discover the full versions of our APIs please visit: https://products.aspose.com/words/

Mikheev Periods, Capitalized Words, etc.
is used together with the positive evidence (proper name) and blocks assignment when con icts are found. For instance, if the system detects a capitalized phrase ThePresident in an unambiguous position, then the sequence strategy will treat the word the as part of the proper name The President even when this phrase follows a period. If in the same document, however, the system detects alternative evidence (e.g., the President, where the is not part of the proper name), it then will block as unsafe the assignment
of The as a proper name in ambiguous usages of The President.
7.2 Frequent-List Lookup Strategy
The frequent-list lookup strategy applies lookup of ambiguously capitalized words in two word lists. The rst list contains common words that are frequently found in sentence-starting positions, and the other list contains the most frequent proper names. Both these lists can be compiled completely automatically, as explained in section 5. Thus, if an ambiguous capitalized word is found in the list of frequent sentence-starting common words, it is assigned as a common word, and if it is found in the list of frequent proper names, it is assigned as a proper name. For instance, the word token The when used after a period will be recognized as a common word, because The is a frequent sentence-starting common word. The Word token Japanin a similar context will be recognized as a proper name, because Japanis a member of the frequent-proper-name list.
Note, however, that this strategy is applied after the sequence strategy and thus, a word listed in one of the lists will not necessarily be marked according to its list class. The list lookup assignment is applied only to the ambiguously capitalized words that have not been handled by the sequence strategy.
This document was truncated here because it was created in the Evaluation Mode.
Created with an evaluation copy of Aspose.Words. To discover the full versions of our APIs please visit: https://products.aspose.com/words/
303
